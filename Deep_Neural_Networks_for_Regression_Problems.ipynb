{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Neural Networks for Regression Problems.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guilhen/NN_forecast/blob/master/Deep_Neural_Networks_for_Regression_Problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGTpd6g0r_r",
        "colab_type": "text"
      },
      "source": [
        "# Deep Neural Networks for Regression problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJr9iTt600FD",
        "colab_type": "text"
      },
      "source": [
        "Neural networks are well known for classification problems, for example, they are used in handwritten digits classification, but the question is will it be fruitful if we used them for regression problems?\n",
        "\n",
        "In this article I will use a deep neural network to predict house pricing using a dataset from KaggleÂ .\n",
        "\n",
        "You can download the dataset from [Here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTGH2f362N2L",
        "colab_type": "text"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/2000/1*vUKwarc7rCouMzSt0Ksakw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itv6SMLe6evK",
        "colab_type": "text"
      },
      "source": [
        "## Contents :\n",
        "    1- Process the dataset\n",
        "    2- Make the deep neural network\n",
        "    3- Train the DNN\n",
        "    4- Test the DNN\n",
        "    5- Compare the result from the DNN to another ML algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFZCb922_jfw",
        "colab_type": "text"
      },
      "source": [
        "**First of all, we will import the needed dependencies :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9QlV0BC_9PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvdfBOX87gGV",
        "colab_type": "text"
      },
      "source": [
        "## First : Processing the dataset \n",
        "We will not go deep in processing the dataset, all we want to do is getting the dataset ready to be fed into our models .\n",
        "\n",
        "We will get rid of any features with missing values, then we will encode the categorical features, that's it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neKAXXozAmf9",
        "colab_type": "text"
      },
      "source": [
        "### Load the dataset :\n",
        "* Load train and test data into pandas DataFrames\n",
        "* Combine train and test data to process them together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeLRCvK6Au23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    #get train data\n",
        "    train_data_path ='train.csv'\n",
        "    train = pd.read_csv(train_data_path)\n",
        "    \n",
        "    #get test data\n",
        "    test_data_path ='test.csv'\n",
        "    test = pd.read_csv(test_data_path)\n",
        "    \n",
        "    return train , test\n",
        "\n",
        "def get_combined_data():\n",
        "  #reading train data\n",
        "  train , test = get_data()\n",
        "\n",
        "  target = train.SalePrice\n",
        "  train.drop(['SalePrice'],axis = 1 , inplace = True)\n",
        "\n",
        "  combined = train.append(test)\n",
        "  combined.reset_index(inplace=True)\n",
        "  combined.drop(['index', 'Id'], inplace=True, axis=1)\n",
        "  return combined, target\n",
        "\n",
        "#Load train and test data into pandas DataFrames\n",
        "train_data, test_data = get_data()\n",
        "\n",
        "#Combine train and test data to process them together\n",
        "combined, target = get_combined_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPcvLGQGjWSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL7U515m9BXQ",
        "colab_type": "text"
      },
      "source": [
        "let's define a function to get the columns that don't have any missing values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwHRdGTD9YRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cols_with_no_nans(df,col_type):\n",
        "    '''\n",
        "    Arguments :\n",
        "    df : The dataframe to process\n",
        "    col_type : \n",
        "          num : to only get numerical columns with no nans\n",
        "          no_num : to only get nun-numerical columns with no nans\n",
        "          all : to get any columns with no nans    \n",
        "    '''\n",
        "    if (col_type == 'num'):\n",
        "        predictors = df.select_dtypes(exclude=['object'])\n",
        "    elif (col_type == 'no_num'):\n",
        "        predictors = df.select_dtypes(include=['object'])\n",
        "    elif (col_type == 'all'):\n",
        "        predictors = df\n",
        "    else :\n",
        "        print('Error : choose a type (num, no_num, all)')\n",
        "        return 0\n",
        "    cols_with_no_nans = []\n",
        "    for col in predictors.columns:\n",
        "        if not df[col].isnull().any():\n",
        "            cols_with_no_nans.append(col)\n",
        "    return cols_with_no_nans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkdPBeu1n2Se",
        "colab_type": "text"
      },
      "source": [
        "Get the columns that do not have any missing values ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdH1rGZFjNL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_cols = get_cols_with_no_nans(combined , 'num')\n",
        "cat_cols = get_cols_with_no_nans(combined , 'no_num')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frf_IY8Onvqm",
        "colab_type": "text"
      },
      "source": [
        "Let's see how many columns we got"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X295Nvfl5ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('Number of numerical columns with no nan values :',len(num_cols))\n",
        "print ('Number of nun-numerical columns with no nan values :',len(cat_cols))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11f-e_7dmhRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined = combined[num_cols + cat_cols]\n",
        "combined.hist(figsize = (12,10))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N793AWuKeS7v",
        "colab_type": "text"
      },
      "source": [
        "**The correlation between the features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ_lUgcCodbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_data[num_cols + cat_cols]\n",
        "train_data['Target'] = target\n",
        "\n",
        "C_mat = train_data.corr()\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "\n",
        "sb.heatmap(C_mat, vmax = .8, square = True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2A_bc2tqLf7",
        "colab_type": "text"
      },
      "source": [
        "From the correlation heatmap above, we see that about 15 features are highly correlated with the target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV0ziidQqnE3",
        "colab_type": "text"
      },
      "source": [
        "**One Hot Encode The Categorical Features :**\n",
        "\n",
        "We will encode the categorical features using one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo2ZI9barWSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def oneHotEncode(df,colNames):\n",
        "    for col in colNames:\n",
        "        if( df[col].dtype == np.dtype('object')):\n",
        "            dummies = pd.get_dummies(df[col],prefix=col)\n",
        "            df = pd.concat([df,dummies],axis=1)\n",
        "\n",
        "            #drop the encoded column\n",
        "            df.drop([col],axis = 1 , inplace=True)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng_BSoDwrr3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('There were {} columns before encoding categorical features'.format(combined.shape[1]))\n",
        "combined = oneHotEncode(combined, cat_cols)\n",
        "print('There are {} columns after encoding categorical features'.format(combined.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MosRPZPEtEgt",
        "colab_type": "text"
      },
      "source": [
        "Now, split back combined dataFrame to training data and test data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INvgC50TtPmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_combined():\n",
        "    global combined\n",
        "    train = combined[:1460]\n",
        "    test = combined[1460:]\n",
        "\n",
        "    return train , test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZL6rBHUtYDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = split_combined()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPKYTh4wtinr",
        "colab_type": "text"
      },
      "source": [
        "## Second : Make the Deep Neural Network\n",
        " * Define a sequential model\n",
        " * Add some dense layers\n",
        " * Use '**relu**' as the activation function in the hidden layers\n",
        " * Use a '**normal**' initializer as the kernal_intializer \n",
        "           Initializers define the way to set the initial random weights of Keras layers.\n",
        " * We will use mean_absolute_error as a loss function\n",
        " * Define the output layer with only one node\n",
        " * Use 'linear 'as the activation function for the output layer\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsJsrFlIvmzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrZVKcMbwCcI",
        "colab_type": "text"
      },
      "source": [
        "**The Input Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILFBftZnvqFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6RW_lRRwG2i",
        "colab_type": "text"
      },
      "source": [
        "**The Hidden Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz61h9vLv7xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHSMp0zJwKRc",
        "colab_type": "text"
      },
      "source": [
        "**The Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9v_kTrsv-38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHpg10glxNam",
        "colab_type": "text"
      },
      "source": [
        "**Compile the network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROXr07cAv-pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMirCFkrxUA8",
        "colab_type": "text"
      },
      "source": [
        "**Define a checkpoint callback :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4kZNz7HxafP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgT8VgWxicp",
        "colab_type": "text"
      },
      "source": [
        "## Third : Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGEt2nCHzMZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7waTYZD9QDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = 'Weights-478--18738.19831.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ecLK0Zp7WlE",
        "colab_type": "text"
      },
      "source": [
        "We see that the validation loss of the best model is 18738.19 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Y42dnt8evM",
        "colab_type": "text"
      },
      "source": [
        "## Fourth : Test the model\n",
        "We will submit the predictions on the test data to Kaggle and see how good our model is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn71O0NRK_w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_submission(prediction, sub_name):\n",
        "  my_submission = pd.DataFrame({'Id':pd.read_csv('test.csv').Id,'SalePrice':prediction})\n",
        "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
        "  print('A submission file has been made')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_XboCvj9Js1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = NN_model.predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRUZk-Ir-_dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_submission(predictions[:,0],'submission(NN).csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBLw2KoPAq3k",
        "colab_type": "text"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*mXbUrGB9yB9RrBscwugP9g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_LWqRyKEEFo",
        "colab_type": "text"
      },
      "source": [
        "Not bad at all, with some more preprocessing, and more training, we can do better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vojrVZ9LEkgc",
        "colab_type": "text"
      },
      "source": [
        "## Fifth: Try another ML algorithms :\n",
        "Now, let us try another ML algorithm to compare the results.\n",
        "\n",
        "We will use random forest regressor and XGBRegressor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQwnJ4d-b1WK",
        "colab_type": "text"
      },
      "source": [
        "**Split training data to training and validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIKPQXCbE4fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(train, target, test_size = 0.25, random_state = 14)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vHJ0gzZb_ri",
        "colab_type": "text"
      },
      "source": [
        "**We will try Random forest model first.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoegrtWbGGM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RandomForestRegressor()\n",
        "model.fit(train_X,train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5lsFUEPcGft",
        "colab_type": "text"
      },
      "source": [
        "**Get the mean absolute error on the validation data **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DovxifwFFetV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_prices = model.predict(val_X)\n",
        "MAE = mean_absolute_error(val_y , predicted_prices)\n",
        "print('Random forest validation MAE = ', MAE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-nsd2pGcUZ-",
        "colab_type": "text"
      },
      "source": [
        "**Make a submission file and submit it to Kaggle to see the result.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3IFk9GWHP-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_prices = model.predict(test)\n",
        "make_submission(predicted_prices,'Submission(RF).csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoyvxR6bx84",
        "colab_type": "text"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*wcJqKzBsbLARqMjwvv0Xjw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgZlYnjWcgqk",
        "colab_type": "text"
      },
      "source": [
        "**Now, let us try XGBoost model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZi76ZV1GKLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGBModel = XGBRegressor()\n",
        "XGBModel.fit(train_X,train_y , verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSIqN9Pdc0Hr",
        "colab_type": "text"
      },
      "source": [
        "**Get the mean absolute error on the validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuL_T225GboJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGBpredictions = XGBModel.predict(val_X)\n",
        "MAE = mean_absolute_error(val_y , XGBpredictions)\n",
        "print('XGBoost validation MAE = ',MAE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMh7f_2pc9rf",
        "colab_type": "text"
      },
      "source": [
        "**Make a submission file and submit it to Kaggle to see the result.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipV7tGfOMaGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGBpredictions = XGBModel.predict(test)\n",
        "make_submission(XGBpredictions,'Submission(XGB).csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q2SPfmZbvcz",
        "colab_type": "text"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*PO0jxykz1hv-aSN5kkjItg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rysvLk6Yc_wW",
        "colab_type": "text"
      },
      "source": [
        "Isn't that a surprise, I really did not think that neural networks will beat random forests and XGBoost algorithms, but let us try not to be too optimistic, remember that we did not configure any hyperparameters on random forest and XGBoost models, I believe if we did so, these two models would outscore neural networks."
      ]
    }
  ]
}